---
title: Insert title here
key: 428af9dcb37795f7be345b5281397cda

---
## Introduction to PySpark RDD

```yaml
type: "TitleSlide"
key: "2fe2adb764"
```

`@lower_third`

name: Upendra Devisetty
title: Science Analyst


`@script`



---
## RDDs in PySpark

```yaml
type: "FullSlide"
key: "fe4f59f0a1"
```

`@part1`
- What is RDD?

- Creating RDDs using Parallelize method

- Creating RDDs from External datasets

- Creating RDDs from Existing RDDs

- Transformations (Map, Filter, FlatMap)

- Actions (Collect, Take, First, Count)


`@script`



---
## What is RDD?

```yaml
type: "FullSlide"
key: "c808f99a10"
```

`@part1`
- Data in Spark is organized into RDD, DataFrames and Datasets

- Fundamental and backbone of Spark

- Immutable distributed collection of records, partitioned across nodes in the cluster

- **R**esilient **D**istributed **D**atasets

- **Resilient** - Ability to withstand failures 

- **Distributed** - Spanning across multiple machines


`@script`



---
## Creating RDDs

```yaml
type: "FullSlide"
key: "8c4096e03c"
center_content: false
disable_transition: false
```

`@part1`
- ParallelizingÂ an existing collection


`@script`
numRDD = sc.parallelize([1,2,3,4])


---
## Final Slide

```yaml
type: "FinalSlide"
key: "a59e80f8a0"
```

`@script`


